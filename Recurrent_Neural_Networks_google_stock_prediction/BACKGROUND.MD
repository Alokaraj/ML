# rnn-google-stock-prediction

Recurrent Neural Networks
- venturing into complex, forward-looking and cutting-edge of Deep Learning

Outline: 
- The idea behind Recurrent Neural Networks, compare them to the human brain and what makes them special in comparison to regular ANN.
- The Vanishing Gradient Problem - used to prevent them to moving forward
- Long Short-Term Memory (LSTM) - architecture, very exciting, complex structure
- Practical Intuition - examples of researchers, intuitive, how they'll think (neuroscience)
- Extra -> LTSM Variations

- Supervised Deep Learning 
  - Artificial Neural Networks (ANN): Used for Regression and Classification
  - Convolutional Neural Networks (CNN): Used for Computer Vision
  - Recurrent Neural Networks (RNN): Used for Time Series Anaylsis
- Unsupervised Deep Learning
  - Self-Organizing Maps: Used for Feature Detection
  - Deep Boltzmann Machines: Used for Recommendation Systems
  - AutoEncoders: Used for Recommendation Systems

- Idea behind Recurrent Neural Networks (RNN)
  - RNN is one of the most advanced algorithms for Supervised Deep Learning
  - Human Brain - where we're in the map of the brain
    - Deep Learning is to mimick the human brain and get similar functions and leverage what evolution already has for us
    - Brain 
      - Cerebrum 
        - Frontal Lobe
          - Recurrent Neural Networks
          - Frontal lobe responsible for short-term memory, personality, motor cortex, behavior, working memory
        - Temporal Lobe
          - Artificial Neural Networks
          - Temporal responsible for long-term memory
          - weights represent long-term memory
          - Recognition memory (long-term)
        - Parietal Lobe
          - Sensation and perception and creating a spatial coordination system represent the world around them
        - Occipital Lobe
          - Convolutional Neural Networks (CNN)
      - Cerebellum (Latin for little brain)
      - Brainstem - connects brain to organ
      - ANN main advantage? breakthrough, aside from backpropagation (apply everything)
        - the weights, can learn through prior experience (observation, epochs)
        - weights are present in all neurons in the brain
        - weights represent long-term memory, process the same tomorrow, etc.
   - RNN is like short-term memory, they can remember things that just happened in the previous couple of observations and apply that knowledge going forward
   - Neural Networks
    - Input Layer, Hidden Layer, Output Layer
    - How to change to RNN? Squash everything, think of it looking underneath neural network, new dimension, flatten out, neurons still there, twist the entire thing and make it vertical
    - Hidden layer - temporal loop, not only gives an output but feeds itself, common approach to unwind loop, whole layer of neurons
    - Neurons connecting to each through time, previous neurons
    - Allows them to pass information to themselves in the future and analyze them
  - Examples
    - One-to-Many Relationship
      - One input and many outputs
      - Image where computer describes an image
        - CNN -> RNN, come up with description
        - Long-term memory, feature recognition system, RNN makes sense out of the sentence ('black and white dog jumps over bar... nouns and verbs)
    - Many to One 
      - Sentiment anaylsis 
        - Lots of text, is it positive comment or negative comment 
    - Many to Many
      - Google Translator 
      - Gender (languages, one replacement, changes the sentence, depend on the words of the changed, short-term of the previous word to know the next)
      - Subtitle movies
      - RNN movie, LTSM written by Benjamin - https://www.youtube.com/watch?v=LY7x2Ihqjmc (9 Minutes)
      - https://arstechnica.com/gaming/2016/06/an-ai-wrote-this-movie-and-its-strangely-moving/
      - Construct sentences for the most parts, lacks a bigger picture, seperate sentences (90% makes sense) but linking sentences together

- Vanishing Gradient Problem
  - Cost Function, Gradient Descent C = 1/2 (y^ - y)^2
  - Win, Wrec, Wout
  - RNN's Node is a representation of a whole layer
  - Cost function compares what you should be getting
  - Wrec = weight recurring, multiplying the weights multiple time
  - Problem arises: multiplying small, it gets smaller, weights assigned close to zero; if Wrec starts close to zero, gradient becomes even less
  - Vanishing gradident is bad the lower the gradient the harder the network can update the weights
  - The lower the gradient is slower, the higher the gradient the faster it can update the weights
  - viscious cycle, training is slow, training on the disbalance, the whole network is not being trained correctly because of the weights. Domino effect
  - Wrec ~ small => Vanishing
  - Wrec ~ large => Exploding
  - Solutions:
    1. Exploding Gradient
      - Truncated Backpropagation
      - Penalties 
      - Gradient Clipping (maximum value)
    2. Vanishing Gradient
      - Weight initialization 
      - Echo State Networks
      - Long Short-Term Memory Networks (LSTMs)
  - Additional Reading:
    - Sepp (Josef) Hochreiter, 1991, Untersuchungen zu dynamischen neuronalen Netzen
    - Yoshua Bengio, 1994, Learning Long-Term Dependencies with Gradient Descent is Difficult
    - Recommend: Razvan Pascanu, 2013, On the difficulty of training recurrent neural networks - http://proceedings.mlr.press/v28/pascanu13.pdf
- Long Short-term Memory
  - Outline
    - A bit of history
    - LSTM Architecture
    - Example Walkthrough
  - History
    - Vanishing gradient problem: propogate the error through the network it goes through the unraveled temporal loop and as it does it goes through these layers of neurons which are conencted to themselves. These hidden layers which are conntected to themselves and they're connect by the means called the W recurrent weight and because the weight is applied many many times on top of itself that causes the graident to decline rapidly meaning the weight of the layers on very far left are a bit dated and much slower than the weights on the other layers on the far right and this creates a domino effect. 
    - It creates a domino effect because the weights on the far left layers are vary important because they dictate the outputs of those layers which are the inputs to the far right layers and therefore the whole training of the network suffers; thus it's called the problem of the vanishing gradient.
    - Echo State Networks, LSTMs - seperate yourself from theory and knowledge, how'd you solve this?
    - Well make Wrec = 1, LSTMs, and that's all it took to get rid of the vanishing gradient descent problem
  - What is the Long Short-Term Memory (LSTMs)
    - Christopher Olah, 2015, Understanding LSTM Networks
    - Great read - https://colah.github.io/posts/2015-08-Understanding-LSTMs/
    - Main point Wrec = 1 - pipeline at the top (two pointwise operation, no complex neural network operation)
    - Main point LSTM have a memory cell (pipeline) and goes through time and freely flow through time, sometimes it might be removed or added, backpropagate you don't have the vanishing gradient 
    - C memory cell, H is the output
    - Everthing here is a vector (lots of values behind Xt and everywhere as they're layers (vector))
    - Vector transfer
    - Copy, pointwise operation 
      - (x = valves, open or close)
      - forget valve (open closed, memory is closed)
      - sigma - sigmoid activation (memory valve)
      - T-shaped joint
      - tangent function
      - Neural network layer (going in and out, layer operations)
      - pointwise (point by point, pointless operation)
    1. new value coming in (ht-1, xt) whether to open or close valve
    2. one layer operation
    - Example Google Translate
      - Store subject (boy) flows through freely 
      - New subject (girl, amanda)
    - Additional Readings:
      - Sepp Hochreiter & Jurgen Schmidhuber, 1997, Long Short-Term Memory
      - Christopher Olah, 2015, Understanding LSTM Networks
      - Shi Yan, 2016, Understanding LSTM and its diagrams
- How LSTM works in practical application
  - LSTM function, tangent function, and then fires up
  - How LSTM works in practical application
  - First look at tanh function and how it fires up
    - minus 1 red, +1 is blue according to article
    - Predict text what is coming, War and Peace - Tolstoy
    - Activated for URLs
    - Andrej Karpathy, 2015, The Unreasonable Effectiveness of Recurrent Neural Networks - https://karpathy.github.io/2015/05/21/rnn-effectiveness/
    - Andrej Karpathy, 2015, Visualizing and Understanding Recurrent Networks - completely tink on its own, seperate being, mysterious something magical, decade think completely on its own.
- Variations of LSTMs
  - Standard LSTMs 
  - Variation #1 add peepholes
    - connecting sigmoid of the neural network, current state of the memory cell, allow decision about the valves and what's sitting
  - Variation #2 Connect forget valve and memory valve, combined decision
  - Variation #3 Gated Recurrent Units (GRUs) => remove memory pipeline - simplifies things
  - Klaus Greff, 2015, LSTM: A Search Space Odyssey - https://arxiv.org/pdf/1503.04069.pdf

- The Unreasonable Effectiveness of Recurrent Neural Networks
  - Image captioning - generate very nice looking description of images that on the edge of amking sense
  - Common wisdom is that RNNs were supposed to be difficult to train
  - Magical outputs, witnessed their power and robustness many times, still amuses 
  - Character-level language models based on multi-layer LSTMs
  - What are Recurrent Neural Networks?
    - Limitation of Vanilla Neural Networks (Convolution Networks) - API too constrained - accepted a fixed-sized vector as input (e.g.) an image and produce a fixed-sized vector as output (probabilities of different classes) and uses a fixed amount of computational steps (number of layers in the model)
    - Recurrent Nets are more exciting allow us to operate over "sequences" of vectors: sequences in the input, output, or in the most general case both. 
      - One-to-One (Input Vector -> matrix multiply -> Output Vector) - Vanilla mode of processing without RNN, from fixed-sized input to fixed-size output (e.g. image classification)
      - One-to-Many (Sequence output) - e.g. image captioning takes an image and ouputs a sentnece of words
      - Many-to-One (Sequence input) e.g. sentimenet anaylsis where a given sentence is classified as expressing positive or negative sentiment
      - Many-to-Many (Sequence input and sequence output) e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French
      - Many-to-Many - Synced sequence input and output e.g. video classification where we wish to label each frame of the video
  - RNNs combine the input vector and their state vector with a fixed (but learned) function to produce a new state vector.
  - RNNS are Turing-Complete in the sense that they can be used to simulate arbitraty programs (with proper weights) - but similar to universal approximation theorms, neural nets - don't look too much into this.
  - If you data is not in form of sequences, you can still formulate and train powerful models that learn to process it sequentially.
  - RNN Computation
    - How do these things work? 
    - At the core, RNNs have a deceptively simple API -> accept an input vector x and gives you an output vector y
    - Output vectors contents are influenced not only by the input just fed in, but also on the entire history of inputs you've fed in the past.
    - `rnn = RNN()`
    - `y = rnn.step(x) # x is an input vector, y is the RNN's output vector`
    - RNNs are neural networks and everything works monotonically better (if done right) if you put on your deep learning hat and start stacking models up like pancakes. For instance, we can form a 2-layer recurrent network as follows:
      - `y1 = rnn1.step(x)`
      - `y = rnn2.step(y1)`  
  - LSTMs
    - Long Short-Term Memory (LSTM) network. The LSTM is a particular type of recurrent network that works slightly better in practice, owing to its more powerful update equation and some appealing backpropagation dynamics
      - I won’t go into details, but everything I’ve said about RNNs stays exactly the same, except the mathematical form for computing the update (the line self.h = ... ) gets a little more complicated. From here on I will use the terms “RNN/LSTM” interchangeably but all experiments in this post use an LSTM.
 - Summary of RNN and LSTMs
  - 1. History
    - AI has been around and in the background for decades. In 2017 AI has broken through and arrived in a big way.
    - Why though? What's the big deal all of a sudden? What do Reccurent Neural Networks have to with it? Well, a lot, actually
    - Thanks to an ingeious form of short-term memory that is unheard of in conventional neural networks, RNNs have been proving themselves as powerful predictive engines.
    - When it comes to sequential machine learning tasks, such as speech recognition, RNNs are reaching levels of predictive accurarcy, time and time again, that no other algorithm can match.
    - First RNNs were no hot suffering from a serious setback in their error-tweaking process that held up their progress for decades
    - A major breathrough in the 90s led to a new generation of far more accurate RNNs - Google Voice Search, Siri, igniting the AI renaissance that's unfoleding now
   - 2. Neural Networks that Cling to the Past
      - Most ANN such as feedforward neural networks, have no memory of the input they received just a moment ago.
      - E.g. if you provide a feedforward neural network with the sequence of letters "WISDOM" when it gets to "D" it has already forgotten "S" => Which is a big problem, as no matter how hard you train it, it will always struggle to guess the most likely next character 
      - Horrible for speech recognition which greatly benefit from the capacity to predict what's coming next
      - Recurrent Neural Networks, on the other hand, do remember what they've just encountered, and at a remarkably sophisticated level
   - 3. Applying "WISDOM" to Recurrent Neural Networks (RNNs)
      - The unit, or artificial neuron, of the RNN, upon receiving the "D" also takes as its input the character it received one moment ago, the "S"
      - In other words, it adds the immediate past to the present.
      - This gives it the advantage of limited short-term memory that, along with its training, provides enough context for guessing what the next character is most likely to be: "O"
   - 4. Tweaking and Re-Tweaking
      - Like all ANN, units of an RNN assign a matrix of weights to their multiple inputs, then apply a function to those weights to determine a single output.
      - However, recurrent neural networks apply weights not only to their present inputs, but also to their inputs from moments ago.
      - Then adjust the weights assigned to their present and past inputs through a process that involves two concepts you'll definitely want to know if you want to get into AI:
        - Gradient Descent
        - Backpropagation through time (BPTT)
   - 5. Gradient Descent
      - Gradient Descent is one of the most famous algorithm in Machine Learning
      - Its priamry virtue is its remarkable capacity to sidestep the dreaded "curse of dimensionality" 
      - Issue plagues systems such as neural networks, with far too many variables to make a brute-force calculation
      - Gradient descent breaks the curse of dimensionaility by zooming in on the local low-point or local minimum of the multi-dimensional error or cost function
      - This helps the system determine the tweaked value, or weight, to assign to each units in the network, bringing accuracy back in line.
   - 6. Backpropogation Through Time
      - RNN trains its units by adjusting their weights through a slight modification of a feedback process known as backpropogation
      - Works its way back, layer by layer, from network's final output, tweaking the weights of each unit or artificial neuron, according to the units calculated portion of the total output error
      - Recurrent neural networks use a heavier version of this process knwon as backpropogation through time (BPTT) 
      - BPTT extends the tweaking processing to include the weight of the T-1 input values responsible for each unit's memory of the prior moment
    - 7. Vanishing Gradient Problem
      - Initial success with gradient descent and BPTT, many ANNs, including 1st gen RNNs run out of gas
      - Suffer from serious setback known as vanishing gradient problem
      - Basic idea is the following:
        - 1. Notion of a gradient - simpler relative, the derivative, you can think of a gradient as a slope
        - 2. Larger the gradient, the steeper the slope, the more quickly the system can roll downhill to the finish line and complete its training
        - 3. Slopes were too flat for fast training - particularly in the first layers of the deep networks, which are the most critical when it comes to proper tweaking of memory units.
        - 4. They ge so small, that their corresponding slopes so flat, describe as "vanishing"
        - 5. smaller and smaller, thus flatter and flatter, the training times grew unbearably long
    - 8. Long Short-Term Memory
      - 90s a major breakthrough solved the vanishing descent problem and gave a second wind to recurrent network development
      - center of this new approach were units of long short-term memory (LSTM)
      - these artificial neruons remember their inputs from a moment ago. However, unlike standard RNN units, LSTMs can hang on to their memories, which have read/write properties akin to memory registers in a conventional computer.
      - LSTMs have analog, rather than digital, memory - making their functions differentiable
      - In other words, curves are continuous and you canf ind the steepness of their slopes.
      - good fit for partial differential calculus involved in backpropagation and gradient descent
      - LSTMs can not only tweak their weights, but retain, delete, transform, and otherwise control the inflow and outflow of their stored data according to the quirks of their training.
      - LSTMs can cling to important error information for long enough to keep gradients relatively steep and thus training periods relatively short
      - The above wipes out the vanishing gradient problem and greatly improves the accuracy of today's LSTM-based recurrent networks
    - 9. What to remember about RNNs
      - Recurrent Neural Networks can remember their former inputs which gives them a big edge over other artificial neural networks when it comes to sequential, context-sensitive tasks such as speech recognition. 
      - First generation RNN hit the wall when it came to their capacity to correct for errors through the all-important twin proceseses of backpropagation and gradient descent
      - Vanishing Gradient Descent Problem - halted progress in the field until 1997 when LSTM-based architecture were introduced
      - LSTM appraoch effective turned each unit in a recurrent network into an analogue computer greatly increased accuracy and helped lead to the renaissance in AI we're seeing all around us today.
        
