{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer(bag of words)\n",
    "its a algorithm, also a model of feature extraction,\n",
    "which converts text to numeric(0's & 1's),mothing but words to vector.\n",
    "it creats a sparse matrix(less 1's, more 0's)\n",
    "Dense matrix(less 0's, more 1's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example text for model training\n",
    "simple_train=['call you tonight call','call me a cab','please call me','what is the oam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect =CountVectorizer()#creating object of CountVectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model to training data\n",
    "#learn the 'vocablary of the traing data (occurs in-place)\n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'call', 'is', 'me', 'oam', 'please', 'the', 'tonight', 'what', 'you']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine the fitted vocablary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform training data into a 'document-term-matrix'\n",
    "simple_train_dtm=vect.transform(simple_train)\n",
    "#simple_train_dtm = vect.fit_transform(simple_train)\n",
    "#we can also use fit and transform methods at a time.(fit_transform)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting sparse matrix into dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>is</th>\n",
       "      <th>me</th>\n",
       "      <th>oam</th>\n",
       "      <th>please</th>\n",
       "      <th>the</th>\n",
       "      <th>tonight</th>\n",
       "      <th>what</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  is  me  oam  please  the  tonight  what  you\n",
       "0    0     2   0   0    0       0    0        1     0    1\n",
       "1    1     1   0   1    0       0    0        0     0    0\n",
       "2    0     1   0   1    0       1    0        0     0    0\n",
       "3    0     0   1   0    1       0    1        0     1    0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(simple_train_dtm.toarray(),columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation is a process of turning a collection of text documents into numerical feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example test for model testing\n",
    "simple_test=[\"please dont call me\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transforming testing data into a document-term matrix\n",
    "simple_test_dtm=vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>is</th>\n",
       "      <th>me</th>\n",
       "      <th>oam</th>\n",
       "      <th>please</th>\n",
       "      <th>the</th>\n",
       "      <th>tonight</th>\n",
       "      <th>what</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  is  me  oam  please  the  tonight  what  you\n",
       "0    0     1   0   1    0       1    0        0     0    0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine the vocablary and document-term matrix\n",
    "pd.DataFrame(simple_test_dtm.toarray(),columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "vect.fit(train) learns the vocablary  of training data\n",
    "vect.transform(train) uses the fitted vocablary to build a document term matrix from the traing data\n",
    "vect.transform(test) uses the fitted vocablary to build a document term matrix from testing data(and ignore tokens it has never seen before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence tokanizer and word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "EXEMPLE_TEXT = \"hey, how are you, im fine, im awesome. talk to you later\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey, how are you, im fine, im awesome.', 'talk to you later']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(EXEMPLE_TEXT))#prints all the phrase in a sentence.(sentence tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', ',', 'how', 'are', 'you', ',', 'im', 'fine', ',', 'im', 'awesome', '.', 'talk', 'to', 'you', 'later']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(EXEMPLE_TEXT))#prints all the words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "corpus is nothing but set of documents.\n",
    "here we remove the stopwords like: is , the, etc...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"this is so good, showing off the stop words filteration\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence=[]# creating an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', ',', 'showing', 'stop', 'words', 'filteration', 'good', ',', 'showing', 'stop', 'words', 'filteration']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good,showingstopwordsfilteration\n"
     ]
    }
   ],
   "source": [
    "print(''.join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming with python\n",
    "''' stemming is nothing but finding the root od the word, like: importent -> import, beterer -> beter.'''\n",
    "in PorterStemmer their vll be already customized root words logic will be there, but in WordNetLemmatizer we can stem the specific words by mentioning the word along with the (parts of speeach) pos = a(adjective), v(verbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ps.stem(\"Importent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is important to be very carefull while you are playing with dogs. All dogs should be carefully taken care.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "be\n",
      "veri\n",
      "careful\n",
      "while\n",
      "you\n",
      "are\n",
      "play\n",
      "with\n",
      "dog\n",
      ".\n",
      "all\n",
      "dog\n",
      "should\n",
      "be\n",
      "care\n",
      "taken\n",
      "care\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))#printing the stem words of each words in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "running\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"catss\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"running\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet\n",
    "its a class which tells the synonims(words which give same meanings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "#Synset: a set of synonyms that share a common meaning.\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a radio or television show\n"
     ]
    }
   ],
   "source": [
    "#prints the synonim words of program.n.03 definition(n stands for noun)\n",
    "print(wordnet.synset('program.n.03').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plan', 'program', 'programme']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list comprention technique(for loop return in a single line)\n",
    "[str(lemma.name()) for lemma in wordnet.synset('program.n.01').lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did you see his program last night?\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('program.n.03').examples()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrange a program of or for\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synset('program.v.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('male.n.01')\n",
    "w2 = wordnet.synset('female.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['\"', 'the', 'fighting', 'sullivans', '\"', 'contains', 'a', 'major', 'plot', 'development', 'in', 'the', 'last', 'ten', 'minutes', 'that', 'every', 'movie', 'guide', 'has', 'seen', 'fit', 'to', 'give', 'away', '.', 'there', 'was', 'no', 'dramatic', 'tension', 'for', 'me', 'while', 'watching', 'the', 'film', ',', 'as', 'i', 'knew', 'exactly', 'what', 'was', 'going', 'to', 'happen', '.', 'and', 'that', \"'\", 's', 'the', 'worst', 'thing', 'that', 'can', 'happen', 'before', 'viewing', 'a', 'movie', '.', 'because', 'of', 'this', ',', 'i', 'will', 'tread', 'lightly', 'so', 'i', 'don', \"'\", 't', 'ruin', 'it', 'for', 'anyone', 'reading', 'this', 'review', ',', 'and', 'it', 'would', 'be', 'very', 'advisable', 'to', 'avoid', 'all', 'other', 'material', 'regarding', 'this', 'film', 'until', 'after', 'you', 'have', 'seen', 'it', '.', '\"', 'the', 'fighting', 'sullivans', '\"', 'revolves', 'around', 'a', 'family', 'that', 'consists', 'of', 'a', 'mother', ',', 'father', ',', 'sister', ',', 'and', 'five', 'brothers', '.', 'the', 'brothers', 'are', 'very', 'close', ',', 'despite', 'the', 'occasional', 'disagreement', '.', 'they', 'are', 'inseperable', ',', 'and', 'never', 'stray', 'from', 'one', 'another', '.', 'their', 'friendship', 'and', 'loyalty', 'is', 'the', 'foundation', 'of', 'this', 'picture', '.', 'the', 'first', 'half', 'of', 'the', 'movie', 'follows', 'the', 'brothers', 'as', 'young', 'children', ',', 'and', 'their', 'various', 'adventures', '.', 'when', 'four', 'of', 'them', 'get', 'into', 'a', 'fight', 'with', 'some', 'local', 'boys', ',', 'the', 'youngest', 'sullivan', ',', 'who', 'is', 'inside', 'a', 'church', 'at', 'the', 'time', ',', 'comes', 'running', 'out', 'to', 'assist', ',', 'but', 'not', 'before', 'properly', 'exiting', 'the', 'chapel', '.', 'if', 'i', \"'\", 'm', 'being', 'awfully', 'vague', 'about', 'details', ',', 'it', \"'\", 's', 'because', 'what', 'i', 'have', 'told', 'you', 'is', 'essentially', 'the', 'first', 'hour', 'and', 'a', 'half', '.', 'all', 'i', 'can', 'say', 'is', 'go', 'watch', 'this', 'film', ',', 'because', 'it', 'is', 'a', 'fine', 'portrait', 'of', 'a', 'family', 'that', 'sticks', 'together', ',', 'through', 'the', 'good', 'and', 'the', 'bad', '.', 'and', 'after', 'you', 'see', 'the', 'movie', ',', 'you', 'will', 'understand', 'why', 'this', 'review', 'has', 'been', 'written', 'the', 'way', 'it', 'has', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NN'), ('want', 'VBP'), ('to', 'TO'), ('transfer', 'VB'), ('money', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize,Text,pos_tag\n",
    "sentence = \"i want to transfer money\"\n",
    "tokens = word_tokenize(sentence)\n",
    "text = Text(tokens)\n",
    "tags = pos_tag(text)\n",
    "#tags[3]\n",
    "print (tags)#creats list which has tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "VBP\n",
      "TO\n",
      "VB\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tags) -1):\n",
    "    print(tags[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transfer VB\n"
     ]
    }
   ],
   "source": [
    "nouns = \"NN NNP PRP NNS\".split()\n",
    "verbs = \"VB VBD VBZ\".split()\n",
    "for i in range(len(tags) -1):\n",
    "    if tags[i][1] in verbs and tags[i+1][1] in nouns:\n",
    "        print(tags[i][0],tags[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('transfer', 'VB'), ('money', 'NN')]\n",
      "This sentencesays to perform some action: transfer VB\n",
      "[('What', 'WP'), ('is', 'VBZ'), ('oam', 'JJ')]\n",
      "[('I', 'PRP'), ('want', 'VBP'), ('pizza', 'NN')]\n",
      "This sentencesays to perform some action: want VBP\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('playing', 'VBG'), ('cricket', 'NN')]\n",
      "This sentencesays to perform some action: playing VBG\n",
      "[('Yes', 'UH'), ('I', 'PRP'), ('am', 'VBP')]\n",
      "[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('colour', 'NN'), ('of', 'IN'), ('orange', 'NN')]\n",
      "[('Who', 'WP'), ('is', 'VBZ'), ('president', 'NN'), ('of', 'IN'), ('india', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Who', 'WP'),\n",
       " ('is', 'VBZ'),\n",
       " ('president', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('india', 'NN')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importent part\n",
    "#if noun is followed by verb\n",
    "from nltk import word_tokenize,Text,pos_tag\n",
    "def sentence_matcher(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    text = Text(tokens)\n",
    "    tags = pos_tag(text)\n",
    "    print (tags)\n",
    "    nouns = \"NN NNP PRP NNS\".split()\n",
    "    verbs = \"VB VBD VBP VBG\".split()\n",
    "    \n",
    "    #if noun is after the verb then extract noun & verb. \n",
    "    for i in range(len(tags) -1):\n",
    "        if tags[i][1] in verbs and tags[i+1][1] in nouns:\n",
    "            print(\"This sentencesays to perform some action:\",tags[i][0],tags[i][1])\n",
    "    return tags\n",
    "sentence_matcher(\"I want to transfer money\")\n",
    "sentence_matcher(\"What is oam\")\n",
    "sentence_matcher(\"I want pizza\")\n",
    "sentence_matcher(\"I am playing cricket\")\n",
    "sentence_matcher(\"Yes I am\")\n",
    "sentence_matcher(\"What is the colour of orange\")\n",
    "sentence_matcher(\"Who is president of india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "def sanitize_sentence(sent):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('(')\n",
    "    stop_words.add(')')\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    filtered_sent=[]\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(w)\n",
    "    return  ' '.join(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cisco', 'NNP'), ('Nexus', 'NNP'), ('1000V', 'CD'), ('versions', 'NNS'), ('prior', 'RB'), ('5.2', 'CD'), ('1', 'CD'), ('SV3', 'NNP'), ('1.15', 'CD')]\n",
      "[('Cisco', 'NNP'), ('Nexus', 'NNP'), ('3048', 'CD'), ('Switch', 'NNP'), ('version', 'NN'), ('6.0', 'CD'), ('2', 'CD'), ('U6', 'NNP'), ('6', 'CD'), ('prior', 'NN')]\n",
      "[('Cisco', 'NNP'), ('MDS', 'NNP'), ('9000', 'CD'), ('Series', 'NNP'), ('Multilayer', 'NNP'), ('Switches', 'NNP'), ('versions', 'NNS'), ('prior', 'RB'), ('6.2', 'CD'), ('17', 'CD')]\n",
      "[('EMC', 'NNP'), ('Networker', 'NNP'), ('versions', 'NNS'), ('less', 'RBR'), ('9.1', 'CD')]\n",
      "[('EMC', 'NNP'), ('Networker', 'NNP'), ('versions', 'NNS'), ('greater', 'JJR'), ('9.1', 'CD')]\n",
      "[('EMC', 'NNP'), ('Networker', 'NNP'), ('versions', 'NNS'), ('prior', 'RB'), ('9.1', 'CD')]\n",
      "[('Cisco', 'NNP'), ('Catalyst', 'NNP'), ('3560X-24T-L', 'JJ'), ('IOS', 'NNP'), ('versions', 'NNS'), ('prior', 'RB'), ('15.2', 'CD'), ('4', 'CD'), ('E5', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "filtered_sent_list=[\n",
    "    \"Cisco Nexus 1000V versions prior to 5.2(1)SV3(1.15)\",\n",
    "     \"Cisco Nexus 3048 Switch version 6.0(2)U6(6) and prior\",\n",
    "     \"Cisco MDS 9000 Series Multilayer Switches versions prior to 6.2(17)\",\n",
    "     \"EMC Networker versions less than to 9.1\",\n",
    "     \"EMC Networker versions greater than to 9.1\",\n",
    "     \"EMC Networker versions prior to 9.1\",\n",
    "     \"Cisco Catalyst 3560X-24T-L IOS versions prior to 15.2(4)E5\"\n",
    "]\n",
    "for i in range(0,len(filtered_sent_list)):\n",
    "    filtered_sent=sanitize_sentence(filtered_sent_list[i])\n",
    "    sentence_matcher(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cisco', 'NNP'), ('Nexus', 'NNP'), ('1000V', 'CD'), ('versions', 'NNS'), ('prior', 'RB'), ('5.2', 'CD'), ('1', 'CD'), ('SV3', 'NNP'), ('1.15', 'CD')]\n",
      "version found is  5.2\n",
      "[('Cisco', 'NNP'), ('Nexus', 'NNP'), ('3048', 'CD'), ('Switch', 'NNP'), ('version', 'NN'), ('6.0', 'CD'), ('2', 'CD'), ('U6', 'NNP'), ('6', 'CD'), ('prior', 'NN')]\n",
      "[('Cisco', 'NNP'), ('MDS', 'NNP'), ('9000', 'CD'), ('Series', 'NNP'), ('Multilayer', 'NNP'), ('Switches', 'NNP'), ('versions', 'NNS'), ('prior', 'RB'), ('6.2', 'CD'), ('17', 'CD')]\n",
      "version found is  6.2\n",
      "[('EMC', 'NNP'), ('Networker', 'NNP'), ('versions', 'NNS'), ('less', 'RBR'), ('9.1', 'CD')]\n",
      "version found is  9.1\n",
      "[('EMC', 'NNP'), ('Networker', 'NNP'), ('versions', 'NNS'), ('greater', 'JJR'), ('9.1', 'CD')]\n",
      "version found is  9.1\n",
      "[('EMC', 'NNP'), ('Networker', 'NNP'), ('versions', 'NNS'), ('prior', 'RB'), ('9.1', 'CD')]\n",
      "version found is  9.1\n",
      "[('Cisco', 'NNP'), ('Catalyst', 'NNP'), ('3560X-24T-L', 'JJ'), ('IOS', 'NNP'), ('versions', 'NNS'), ('prior', 'RB'), ('15.2', 'CD'), ('4', 'CD'), ('E5', 'NN')]\n",
      "version found is  15.2\n"
     ]
    }
   ],
   "source": [
    "def get_version_for_each_rule():\n",
    "    for i in range(0,len(filtered_sent_list)):\n",
    "        filtered_sent=sanitize_sentence(filtered_sent_list[i])\n",
    "        pos_matching=sentence_matcher(filtered_sent)\n",
    "        #similar as noun folloerd by verb ALgo\n",
    "        for item in range(len(pos_matching)-1):\n",
    "            if pos_matching[item][1] in ['RB','JJR','RBR'] and pos_matching[item+1][1] in ['CD']:\n",
    "                print(\"version found is \",pos_matching[item+1][0])\n",
    "get_version_for_each_rule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "hit=wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')\n",
    "print (wn.path_similarity(hit, slap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
